\chapwithtoc{Introduction}
Every day, there are millions of articles published on the internet. Various authors write those
with different backgrounds and opinions. Their goal is, however, the same; to convey the information to the reader.
While the information is usually explicit, there are some cases when it is implicit. Such implicit information 
is usually intentionally used to shape the reader's opinion. However, it's also possible
that the author inserts such implicit information unintentionally. It could
be the result of the author's background or state of the world at the time of writing.
The author's texts might be more pessimistic at the start of the week, while more optimistic before the weekend.
It might also be possible that senior authors write articles slightly differently than their colleagues.
We thus ask ourselves the following question:
\begin{quote}
    \textit{Which and how much implicit information can be extracted from the news articles?}
\end{quote}

As there could be an infinite amount of such fingerprints, we narrow the scope of our research to the following tasks:
\begin{enumerate}
    \item Article publishing server
    \item Article category
    \item Author's textual gender
    \item Publication day of week
\end{enumerate}
Furthermore, we limit the scope of the research to the Czech language.

For simplicity, we will further refer
to \textit{Article publishing server} as \textbf{Server},
to \textit{Article category} as \textbf{Category},
to \textit{Author's textual gender} as \textbf{Gender}\footnote{Note that we by no means are referring to the author's actual gender},
and to \textit{Publication day of week} as \textbf{Day Of Week}


\section*{Related Work}
With the rise of \ac{nlp} and \ac{ml},
there has been a lot of research on implicit information in textual content, most notably on sentiment analysis
and text classification. Most notably, \textcite{joulinBagTricksEfficient2016} showed that a simple \acl{bow} approach
with a few hidden layers could achieve \ac{sota} results while being very fast.
\textcite{zhangTextUnderstandingScratch2016}, inspired by the usage of \acp{cnn} in image classification,
proposed a similar approach for text classification with excellent results.

In recent years, \ac{sota} results have been achieved by the transformer models~\parencite{vaswaniAttentionAllYou2017d}.
Such an architecture was used by BERT~\parencite{devlinBERTPretrainingDeep2019a}, where the authors achieved \ac{sota} results on
all the tasks of GLUE benchmark~\parencite{wangGLUEMultiTaskBenchmark2018}, which among others, includes 
sentiment analysis task.

To cope with BERT English mono-lingual training, \textcite{strakaRobeCzechCzechRoBERTa2021}, followed by \textcite{leheckaComparisonCzechTransformers2021},
proposed Czech versions of BERT, called RobeCzech and Fernet-News, respectively.


\section*{Our approach}
We evaluate the defined tasks on CZEch NEws Classification dataset (CZE-NEC), 
the large dataset of Czech news articles of over 1.6 million articles, described in \autoref{chap:dataset}.
To assess human ability on the tasks, we take a subset of the dataset and ask humans to perform the tasks
~(\autoref{sec:human}).

We continue by training a \acl{mlr} model with \acl{bow} features to establish a keyword extraction performance (\autoref{sec:baseline-exp}).
Next, we fine-tune \ac{dl} models,
specifically the RobeCzech~\parencite{strakaRobeCzechCzechRoBERTa2021}
and Fernet-News~\parencite{leheckaComparisonCzechTransformers2021} models,
using various approaches to enhance their performance (\autoref{sec:finetuning}).

Lastly, we explore the multi-lingual capabilities of the commercial Large Language Model GPT-3~\parencite{brownLanguageModelsAre2020b}
by fine-tuning it on CZE-NEC (\autoref{sec:gpt-3}).

\section*{Acknowledgements}
The Github Copilot\footnote{\url{https://github.com/features/copilot}}
was used when writing all source codes including training, data analysis and crawlers.
Grammarly\footnote{\url{https://app.grammarly.com/}} was used to check the grammar and spelling of the thesis.