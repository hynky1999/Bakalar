@inproceedings{daconDoesGenderMatter2021,
  title = {Does {{Gender Matter}} in the {{News}}? {{Detecting}} and {{Examining Gender Bias}} in {{News Articles}}},
  shorttitle = {Does {{Gender Matter}} in the {{News}}?},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2021},
  author = {Dacon, Jamell and Liu, Haochen},
  date = {2021-06-03},
  series = {{{WWW}} '21},
  pages = {385--392},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3442442.3452325},
  url = {https://doi.org/10.1145/3442442.3452325},
  urldate = {2023-02-16},
  abstract = {To attract unsuspecting readers, news article headlines and abstracts are often written with speculative sentences or clauses. Male dominance in the news is very evident, whereas females are seen as “eye candy” or “inferior”, and are underrepresented and under-examined within the same news categories as their male counterparts. In this paper, we present an initial study on gender bias in news abstracts in two large English news datasets used for news recommendation and news classification. We perform three large-scale, yet effective text-analysis fairness measurements on 296,965 news abstracts. In particular, to our knowledge we construct two of the largest benchmark datasets of possessive (gender-specific and gender-neutral) nouns and attribute (career-related and family-related) words datasets1 which we will release to foster both bias and fairness research aid in developing fair NLP models to eliminate the paradox of gender bias. Our studies demonstrate that females are immensely marginalized and suffer from socially-constructed biases in the news. This paper individually devises a methodology whereby news content can be analyzed on a large scale utilizing natural language processing (NLP) techniques from machine learning (ML) to discover both implicit and explicit gender biases.},
  isbn = {978-1-4503-8313-4},
  keywords = {Gender bias,Media bias,News bias},
  file = {/home/kydliceh/Zotero/storage/XHD67JBP/Dacon and Liu - 2021 - Does Gender Matter in the News Detecting and Exam.pdf}
}

@inproceedings{daconDoesGenderMatter2021a,
  title = {Does {{Gender Matter}} in the {{News}}? {{Detecting}} and {{Examining Gender Bias}} in {{News Articles}}},
  shorttitle = {Does {{Gender Matter}} in the {{News}}?},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2021},
  author = {Dacon, Jamell and Liu, Haochen},
  date = {2021-04-19},
  pages = {385--392},
  publisher = {{ACM}},
  location = {{Ljubljana Slovenia}},
  doi = {10.1145/3442442.3452325},
  url = {https://dl.acm.org/doi/10.1145/3442442.3452325},
  urldate = {2023-02-16},
  abstract = {To attract unsuspecting readers, news article headlines and abstracts are often written with speculative sentences or clauses. Male dominance in the news is very evident, whereas females are seen as “eye candy” or “inferior”, and are underrepresented and under-examined within the same news categories as their male counterparts. In this paper, we present an initial study on gender bias in news abstracts in two large English news datasets used for news recommendation and news classification. We perform three large-scale, yet effective textanalysis fairness measurements on 296,965 news abstracts. In particular, to our knowledge we construct two of the largest benchmark datasets of possessive (gender-specific and gender-neutral) nouns and attribute (career-related and family-related) words datasets1 which we will release to foster both bias and fairness research aid in developing fair NLP models to eliminate the paradox of gender bias. Our studies demonstrate that females are immensely marginalized and suffer from socially-constructed biases in the news. This paper individually devises a methodology whereby news content can be analyzed on a large scale utilizing natural language processing (NLP) techniques from machine learning (ML) to discover both implicit and explicit gender biases.},
  eventtitle = {{{WWW}} '21: {{The Web Conference}} 2021},
  isbn = {978-1-4503-8313-4},
  langid = {english},
  file = {/home/kydliceh/Zotero/storage/QTIWKYEK/Dacon and Liu - 2021 - Does Gender Matter in the News Detecting and Exam.pdf}
}

@misc{devlinBERTPretrainingDeep2019a,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2023-01-25},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/SBALYFBN/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/kydliceh/Zotero/storage/CYZ6XEGF/1810.html}
}

@inproceedings{dingCogLTXApplyingBERT2020,
  title = {{{CogLTX}}: {{Applying BERT}} to {{Long Texts}}},
  shorttitle = {{{CogLTX}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ding, Ming and Zhou, Chang and Yang, Hongxia and Tang, Jie},
  date = {2020},
  volume = {33},
  pages = {12792--12804},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/96671501524948bc3937b4b30d0e57b9-Abstract.html},
  urldate = {2023-01-11},
  abstract = {BERTs are incapable of processing long texts due to its quadratically increasing memory and time consumption. The straightforward thoughts to address this problem, such as slicing the text by a sliding window or simplifying transformers, suffer from insufficient long-range attentions or need customized CUDA kernels. The limited text length of BERT reminds us the limited capacity (5∼ 9 chunks) of the working memory of humans – then how do human beings Cognize Long TeXts? Founded on the cognitive theory stemming from Baddeley, our CogLTX framework identifies key sentences by training a judge model, concatenates them for reasoning and enables multi-step reasoning via rehearsal and decay. Since relevance annotations are usually unavailable, we propose to use treatment experiments to create supervision. As a general algorithm, CogLTX outperforms or gets comparable results to SOTA models on NewsQA, HotpotQA, multi-class and multi-label long-text classification tasks with memory overheads independent of the text length.},
  file = {/home/kydliceh/Zotero/storage/VZQUNATZ/Ding et al. - 2020 - CogLTX Applying BERT to Long Texts.pdf}
}

@article{fuksClassificationNewsDataset,
  title = {Classification of {{News Dataset}}},
  author = {Fuks, Olga},
  langid = {english},
  file = {/home/kydliceh/Zotero/storage/SBTVDB54/Fuks - Classiﬁcation of News Dataset.pdf}
}

@article{gaoLimitationsTransformersClinical2021,
  title = {Limitations of {{Transformers}} on {{Clinical Text Classification}}},
  author = {Gao, Shang and Alawad, Mohammed and Young, M. Todd and Gounley, John and Schaefferkoetter, Noah and Yoon, Hong Jun and Wu, Xiao-Cheng and Durbin, Eric B. and Doherty, Jennifer and Stroup, Antoinette and Coyle, Linda and Tourassi, Georgia},
  date = {2021-09},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  volume = {25},
  number = {9},
  pages = {3596--3607},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2021.3062322},
  abstract = {Bidirectional Encoder Representations from Transformers (BERT) and BERT-based approaches are the current state-of-the-art in many natural language processing (NLP) tasks; however, their application to document classification on long clinical texts is limited. In this work, we introduce four methods to scale BERT, which by default can only handle input sequences up to approximately 400 words long, to perform document classification on clinical texts several thousand words long. We compare these methods against two much simpler architectures - a word-level convolutional neural network and a hierarchical self-attention network - and show that BERT often cannot beat these simpler baselines when classifying MIMIC-III discharge summaries and SEER cancer pathology reports. In our analysis, we show that two key components of BERT - pretraining and WordPiece tokenization - may actually be inhibiting BERT's performance on clinical text classification tasks where the input document is several thousand words long and where correctly identifying labels may depend more on identifying a few key words or phrases rather than understanding the contextual meaning of sequences of text.},
  eventtitle = {{{IEEE Journal}} of {{Biomedical}} and {{Health Informatics}}},
  keywords = {Adaptation models,BERT,Biological system modeling,Bit error rate,Cancer,clinical text,Data models,deep learning,MIMICs,natural language processing,neural networks,Task analysis,text classification},
  file = {/home/kydliceh/Zotero/storage/Z9XK9U2S/Gao et al. - 2021 - Limitations of Transformers on Clinical Text Class.pdf;/home/kydliceh/Zotero/storage/BKBGUV3H/9364676.html}
}

@article{haagsmaOverviewCrossGenreGender,
  title = {Overview of the {{Cross-Genre Gender Prediction Shared Task}} on {{Dutch}} at {{CLIN29}}},
  author = {Haagsma, Hessel and Kreutz, Tim and Medvedeva, Masha and Nissim, Malvina},
  abstract = {This overview presents the results of the cross-genre gender prediction task (GxG) organized at CLIN29. Teams were tasked with training a system to predict the gender of authors of tweets, YouTube comments and news articles. In the cross-genre setting, systems were trained on two genres, and tested on the other to assess domain adaptivity of the solutions. Eight teams participated in the shared task. Performance was generally better in the in-genre setting. In the cross-genre settings, performance on news articles declined the most compared to other target genres.},
  langid = {english},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/KYV9VWSK/Haagsma et al. - Overview of the Cross-Genre Gender Prediction Shar.pdf}
}

@article{haagsmaOverviewCrossGenreGendera,
  title = {Overview of the {{Cross-Genre Gender Prediction Shared Task}} on {{Dutch}} at {{CLIN29}}},
  author = {Haagsma, Hessel and Kreutz, Tim and Medvedeva, Masha and Nissim, Malvina},
  abstract = {This overview presents the results of the cross-genre gender prediction task (GxG) organized at CLIN29. Teams were tasked with training a system to predict the gender of authors of tweets, YouTube comments and news articles. In the cross-genre setting, systems were trained on two genres, and tested on the other to assess domain adaptivity of the solutions. Eight teams participated in the shared task. Performance was generally better in the in-genre setting. In the cross-genre settings, performance on news articles declined the most compared to other target genres.},
  langid = {english},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/37BL8YNE/Haagsma et al. - Overview of the Cross-Genre Gender Prediction Shar.pdf}
}

@inproceedings{howardUniversalLanguageModel2018a,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018-07},
  pages = {328--339},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1031},
  url = {https://aclanthology.org/P18-1031},
  urldate = {2023-01-11},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.},
  eventtitle = {{{ACL}} 2018},
  file = {/home/kydliceh/Zotero/storage/PDQ92YD2/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf}
}

@misc{kirosSkipThoughtVectors2015,
  title = {Skip-{{Thought Vectors}}},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  date = {2015-06-22},
  number = {arXiv:1506.06726},
  eprint = {1506.06726},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.06726},
  url = {http://arxiv.org/abs/1506.06726},
  urldate = {2023-01-25},
  abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/JPSIXXGD/Kiros et al. - 2015 - Skip-Thought Vectors.pdf;/home/kydliceh/Zotero/storage/7HBD5D62/1506.html}
}

@misc{liuMultiTaskDeepNeural2019,
  title = {Multi-{{Task Deep Neural Networks}} for {{Natural Language Understanding}}},
  author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  date = {2019-05-29},
  number = {arXiv:1901.11504},
  eprint = {1901.11504},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1901.11504},
  urldate = {2023-01-11},
  abstract = {In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7\% (2.2\% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at https://github.com/namisan/mt-dnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/8Z4BVZ4D/Liu et al. - 2019 - Multi-Task Deep Neural Networks for Natural Langua.pdf;/home/kydliceh/Zotero/storage/38VDFLD6/1901.html}
}

@misc{misraNewsCategoryDataset2022,
  title = {News {{Category Dataset}}},
  author = {Misra, Rishabh},
  date = {2022-10-06},
  number = {arXiv:2209.11429},
  eprint = {2209.11429},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.11429},
  url = {http://arxiv.org/abs/2209.11429},
  urldate = {2023-02-16},
  abstract = {People rely on news to know what is happening around the world and inform their daily lives. In today's world, when the proliferation of fake news is rampant, having a large-scale and high-quality source of authentic news articles with the published category information is valuable to learning authentic news' Natural Language syntax and semantics. As part of this work, we present a News Category Dataset that contains around 210k news headlines from the year 2012 to 2022 obtained from HuffPost, along with useful metadata to enable various NLP tasks. In this paper, we also produce some novel insights from the dataset and describe various existing and potential applications of our dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Read},
  file = {/home/kydliceh/Zotero/storage/VFAA2XV2/Misra - 2022 - News Category Dataset.pdf;/home/kydliceh/Zotero/storage/K7N5SFM4/2209.html}
}

@misc{misraNewsHeadlinesDataset2022,
  title = {News {{Headlines Dataset For Sarcasm Detection}}},
  author = {Misra, Rishabh},
  date = {2022-09-17},
  number = {arXiv:2212.06035},
  eprint = {2212.06035},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2212.06035},
  urldate = {2023-02-16},
  abstract = {Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag-based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets, and detecting sarcasm in these requires the availability of contextual tweets. To overcome the limitations related to noise in Twitter datasets, we curate News Headlines Dataset from two news websites: TheOnion aims at producing sarcastic versions of current events, whereas HuffPost publishes real news. The dataset contains about 28K headlines out of which 13K are sarcastic. To make it more useful, we have included the source links of the news articles so that more data can be extracted as needed. In this paper, we describe various details about the dataset and potential use cases apart from Sarcasm Detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Read},
  file = {/home/kydliceh/Zotero/storage/AXJWQ4YA/Misra - 2022 - News Headlines Dataset For Sarcasm Detection.pdf;/home/kydliceh/Zotero/storage/G8RD2DX3/2212.html}
}

@inproceedings{ruderTransferLearningNatural2019,
  title = {Transfer {{Learning}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Tutorials}}},
  author = {Ruder, Sebastian and Peters, Matthew E. and Swayamdipta, Swabha and Wolf, Thomas},
  date = {2019-06},
  pages = {15--18},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-5004},
  url = {https://aclanthology.org/N19-5004},
  urldate = {2023-01-25},
  abstract = {The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.},
  file = {/home/kydliceh/Zotero/storage/U9HDKXMZ/Ruder et al. - 2019 - Transfer Learning in Natural Language Processing.pdf}
}

@inproceedings{ruderTransferLearningNatural2019a,
  title = {Transfer {{Learning}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Tutorials}}},
  author = {Ruder, Sebastian and Peters, Matthew E. and Swayamdipta, Swabha and Wolf, Thomas},
  date = {2019-06},
  pages = {15--18},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-5004},
  url = {https://aclanthology.org/N19-5004},
  urldate = {2023-01-25},
  abstract = {The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.},
  file = {/home/kydliceh/Zotero/storage/CWU3HMJ3/Ruder et al. - 2019 - Transfer Learning in Natural Language Processing.pdf}
}

@article{seboPerformanceGenderDetection,
  title = {Performance of Gender Detection Tools: A Comparative Study of Name-to-Gender Inference Services},
  shorttitle = {Performance of Gender Detection Tools},
  author = {Sebo, Paul},
  journaltitle = {Journal of the Medical Library Association : JMLA},
  shortjournal = {J Med Libr Assoc},
  volume = {109},
  number = {3},
  eprint = {34629970},
  eprinttype = {pmid},
  pages = {414--421},
  issn = {1536-5050},
  doi = {10.5195/jmla.2021.1185},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8485937/},
  urldate = {2023-02-21},
  abstract = {Objective: To evaluate the performance of gender detection tools that allow the uploading of files (e.g., Excel or CSV files) containing first names, are usable by researchers without advanced computer skills, and are at least partially free of charge. Methods: The study was conducted using four physician datasets (total number of physicians: 6,131; 50.3\% female) from Switzerland, a multilingual country. Four gender detection tools met the inclusion criteria: three partially free (Gender API, NamSor, and genderize.io) and one completely free (Wiki-Gendersort). For each tool, we recorded the number of correct classifications (i.e., correct gender assigned to a name), misclassifications (i.e., wrong gender assigned to a name), and nonclassifications (i.e., no gender assigned). We computed three metrics: the proportion of misclassifications excluding nonclassifications (errorCodedWithoutNA), the proportion of nonclassifications (naCoded), and the proportion of misclassifications and nonclassifications (errorCoded). Results: The proportion of misclassifications was low for all four gender detection tools (errorCodedWithoutNA between 1.5 and 2.2\%). By contrast, the proportion of unrecognized names (naCoded) varied: 0\% for NamSor, 0.3\% for Gender API, 4.5\% for Wiki-Gendersort, and 16.4\% for genderize.io. Using errorCoded, which penalizes both types of error equally, we obtained the following results: Gender API 1.8\%, NamSor 2.0\%, Wiki-Gendersort 6.6\%, and genderize.io 17.7\%. Conclusions: Gender API and NamSor were the most accurate tools. Genderize.io led to a high number of nonclassifications. Wiki-Gendersort may be a good compromise for researchers wishing to use a completely free tool. Other studies would be useful to evaluate the performance of these tools in other populations (e.g., Asian).},
  pmcid = {PMC8485937},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/3H5VGP3R/Sebo - Performance of gender detection tools a comparati.pdf}
}

@inproceedings{strakaSumeCzechLargeCzech2018a,
  title = {{{SumeCzech}}: {{Large Czech News-Based Summarization Dataset}}},
  shorttitle = {{{SumeCzech}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Straka, Milan and Mediankin, Nikita and Kocmi, Tom and Žabokrtský, Zdeněk and Hudeček, Vojtěch and Hajič, Jan},
  date = {2018-05},
  publisher = {{European Language Resources Association (ELRA)}},
  location = {{Miyazaki, Japan}},
  url = {https://aclanthology.org/L18-1551},
  urldate = {2023-02-16},
  eventtitle = {{{LREC}} 2018},
  file = {/home/kydliceh/Zotero/storage/28AVDLW4/Straka et al. - 2018 - SumeCzech Large Czech News-Based Summarization Da.pdf}
}

@misc{sunHowFineTuneBERT2020,
  title = {How to {{Fine-Tune BERT}} for {{Text Classification}}?},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  date = {2020-02-05},
  number = {arXiv:1905.05583},
  eprint = {1905.05583},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1905.05583},
  urldate = {2023-01-10},
  abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/CR77EHN7/Sun et al. - 2020 - How to Fine-Tune BERT for Text Classification.pdf;/home/kydliceh/Zotero/storage/KIDUR7G7/1905.html}
}

@inproceedings{wuMINDLargescaleDataset2020a,
  title = {{{MIND}}: {{A Large-scale Dataset}} for {{News Recommendation}}},
  shorttitle = {{{MIND}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Fangzhao and Qiao, Ying and Chen, Jiun-Hung and Wu, Chuhan and Qi, Tao and Lian, Jianxun and Liu, Danyang and Xie, Xing and Gao, Jianfeng and Wu, Winnie and Zhou, Ming},
  date = {2020-07},
  pages = {3597--3606},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.331},
  url = {https://aclanthology.org/2020.acl-main.331},
  urldate = {2023-02-16},
  abstract = {News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named MIND for news recommendation. Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body. We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. The MIND dataset will be available at https://msnews.github.io.},
  eventtitle = {{{ACL}} 2020},
  file = {/home/kydliceh/Zotero/storage/K6FSKJ2Q/Wu et al. - 2020 - MIND A Large-scale Dataset for News Recommendatio.pdf}
}

@misc{zhuangSurveyEfficientTraining2023,
  title = {A {{Survey}} on {{Efficient Training}} of {{Transformers}}},
  author = {Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
  date = {2023-02-02},
  number = {arXiv:2302.01107},
  eprint = {2302.01107},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.01107},
  urldate = {2023-02-17},
  abstract = {Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/P7JBCFRI/Zhuang et al. - 2023 - A Survey on Efficient Training of Transformers.pdf;/home/kydliceh/Zotero/storage/DSC9XXRY/2302.html}
}
