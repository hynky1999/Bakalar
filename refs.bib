@misc{ActiveMQ,
  title = {{{ActiveMQ}}},
  howpublished = {https://activemq.apache.org/components/artemis/},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/82SYQ2QW/artemis.html}
}

@misc{adhikariDocBERTBERTDocument2019,
  title = {{{DocBERT}}: {{BERT}} for {{Document Classification}}},
  shorttitle = {{{DocBERT}}},
  author = {Adhikari, Ashutosh and Ram, Achyudh and Tang, Raphael and Lin, Jimmy},
  year = {2019},
  month = aug,
  number = {arXiv:1904.08398},
  eprint = {arXiv:1904.08398},
  publisher = {{arXiv}},
  abstract = {We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/YVGNEFPV/Adhikari et al. - 2019 - DocBERT BERT for Document Classification.pdf;/home/kydliceh/Zotero/storage/L793JUZU/1904.html}
}

@inproceedings{adhikariRethinkingComplexNeural2019,
  title = {Rethinking {{Complex Neural Network Architectures}} for {{Document Classification}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Adhikari, Ashutosh and Ram, Achyudh and Tang, Raphael and Lin, Jimmy},
  year = {2019},
  month = jun,
  pages = {4046--4051},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1408},
  abstract = {Neural network models for many NLP tasks have grown increasingly complex in recent years, making training and deployment more difficult. A number of recent papers have questioned the necessity of such architectures and found that well-executed, simpler models are quite effective. We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. Surprisingly, our simple model is able to achieve these results without attention mechanisms. While these regularization techniques, borrowed from language modeling, are not novel, to our knowledge we are the first to apply them in this context. Our work provides an open-source platform and the foundation for future work in document classification.},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/CFU6HBZF/Adhikari et al. - 2019 - Rethinking Complex Neural Network Architectures fo.pdf}
}

@misc{brownLanguageModelsAre2020b,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {arXiv:2005.14165},
  publisher = {{arXiv}},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/Q6T4CZWJ/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/kydliceh/Zotero/storage/SH3G5GES/2005.html}
}

@misc{CommonCrawl,
  title = {Common {{Crawl}}},
  langid = {american},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/HIGRE2IE/commoncrawl.org.html}
}

@inproceedings{daconDoesGenderMatter2021,
  title = {Does {{Gender Matter}} in the {{News}}? {{Detecting}} and {{Examining Gender Bias}} in {{News Articles}}},
  shorttitle = {Does {{Gender Matter}} in the {{News}}?},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2021},
  author = {Dacon, Jamell and Liu, Haochen},
  year = {2021},
  month = jun,
  series = {{{WWW}} '21},
  pages = {385--392},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3442442.3452325},
  abstract = {To attract unsuspecting readers, news article headlines and abstracts are often written with speculative sentences or clauses. Male dominance in the news is very evident, whereas females are seen as ``eye candy'' or ``inferior'', and are underrepresented and under-examined within the same news categories as their male counterparts. In this paper, we present an initial study on gender bias in news abstracts in two large English news datasets used for news recommendation and news classification. We perform three large-scale, yet effective text-analysis fairness measurements on 296,965 news abstracts. In particular, to our knowledge we construct two of the largest benchmark datasets of possessive (gender-specific and gender-neutral) nouns and attribute (career-related and family-related) words datasets1 which we will release to foster both bias and fairness research aid in developing fair NLP models to eliminate the paradox of gender bias. Our studies demonstrate that females are immensely marginalized and suffer from socially-constructed biases in the news. This paper individually devises a methodology whereby news content can be analyzed on a large scale utilizing natural language processing (NLP) techniques from machine learning (ML) to discover both implicit and explicit gender biases.},
  isbn = {978-1-4503-8313-4},
  keywords = {Gender bias,Media bias,News bias},
  file = {/home/kydliceh/Zotero/storage/XHD67JBP/Dacon and Liu - 2021 - Does Gender Matter in the News Detecting and Exam.pdf}
}

@inproceedings{daconDoesGenderMatter2021a,
  title = {Does {{Gender Matter}} in the {{News}}? {{Detecting}} and {{Examining Gender Bias}} in {{News Articles}}},
  shorttitle = {Does {{Gender Matter}} in the {{News}}?},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2021},
  author = {Dacon, Jamell and Liu, Haochen},
  year = {2021},
  month = apr,
  pages = {385--392},
  publisher = {{ACM}},
  address = {{Ljubljana Slovenia}},
  doi = {10.1145/3442442.3452325},
  abstract = {To attract unsuspecting readers, news article headlines and abstracts are often written with speculative sentences or clauses. Male dominance in the news is very evident, whereas females are seen as ``eye candy'' or ``inferior'', and are underrepresented and under-examined within the same news categories as their male counterparts. In this paper, we present an initial study on gender bias in news abstracts in two large English news datasets used for news recommendation and news classification. We perform three large-scale, yet effective textanalysis fairness measurements on 296,965 news abstracts. In particular, to our knowledge we construct two of the largest benchmark datasets of possessive (gender-specific and gender-neutral) nouns and attribute (career-related and family-related) words datasets1 which we will release to foster both bias and fairness research aid in developing fair NLP models to eliminate the paradox of gender bias. Our studies demonstrate that females are immensely marginalized and suffer from socially-constructed biases in the news. This paper individually devises a methodology whereby news content can be analyzed on a large scale utilizing natural language processing (NLP) techniques from machine learning (ML) to discover both implicit and explicit gender biases.},
  isbn = {978-1-4503-8313-4},
  langid = {english},
  file = {/home/kydliceh/Zotero/storage/QTIWKYEK/Dacon and Liu - 2021 - Does Gender Matter in the News Detecting and Exam.pdf}
}

@misc{devlinBERTPretrainingDeep2019a,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {arXiv:1810.04805},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/SBALYFBN/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/kydliceh/Zotero/storage/CYZ6XEGF/1810.html}
}

@inproceedings{dingCogLTXApplyingBERT2020,
  title = {{{CogLTX}}: {{Applying BERT}} to {{Long Texts}}},
  shorttitle = {{{CogLTX}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ding, Ming and Zhou, Chang and Yang, Hongxia and Tang, Jie},
  year = {2020},
  volume = {33},
  pages = {12792--12804},
  publisher = {{Curran Associates, Inc.}},
  abstract = {BERTs are incapable of processing long texts due to its quadratically increasing memory and time consumption. The straightforward thoughts to address this problem, such as slicing the text by a sliding window or simplifying transformers, suffer from insufficient long-range attentions or need customized CUDA kernels. The limited text length of BERT reminds us the limited capacity (5{$\sim$} 9 chunks) of the working memory of humans \textendash{} then how do human beings Cognize Long TeXts? Founded on the cognitive theory stemming from Baddeley, our CogLTX framework identifies key sentences by training a judge model, concatenates them for reasoning and enables multi-step reasoning via rehearsal and decay. Since relevance annotations are usually unavailable, we propose to use treatment experiments to create supervision. As a general algorithm, CogLTX outperforms or gets comparable results to SOTA models on NewsQA, HotpotQA, multi-class and multi-label long-text classification tasks with memory overheads independent of the text length.},
  file = {/home/kydliceh/Zotero/storage/VZQUNATZ/Ding et al. - 2020 - CogLTX Applying BERT to Long Texts.pdf}
}

@misc{EfficientTrainingMultiple,
  title = {Efficient {{Training}} on {{Multiple GPUs}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/perf\_train\_gpu\_many},
  keywords = {/unread}
}

@article{fuksClassificationNewsDataset,
  title = {Classification of {{News Dataset}}},
  author = {Fuks, Olga},
  langid = {english},
  file = {/home/kydliceh/Zotero/storage/SBTVDB54/Fuks - ClassiÔ¨Åcation of News Dataset.pdf}
}

@article{gaoLimitationsTransformersClinical2021,
  title = {Limitations of {{Transformers}} on {{Clinical Text Classification}}},
  author = {Gao, Shang and Alawad, Mohammed and Young, M. Todd and Gounley, John and Schaefferkoetter, Noah and Yoon, Hong Jun and Wu, Xiao-Cheng and Durbin, Eric B. and Doherty, Jennifer and Stroup, Antoinette and Coyle, Linda and Tourassi, Georgia},
  year = {2021},
  month = sep,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {25},
  number = {9},
  pages = {3596--3607},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2021.3062322},
  abstract = {Bidirectional Encoder Representations from Transformers (BERT) and BERT-based approaches are the current state-of-the-art in many natural language processing (NLP) tasks; however, their application to document classification on long clinical texts is limited. In this work, we introduce four methods to scale BERT, which by default can only handle input sequences up to approximately 400 words long, to perform document classification on clinical texts several thousand words long. We compare these methods against two much simpler architectures - a word-level convolutional neural network and a hierarchical self-attention network - and show that BERT often cannot beat these simpler baselines when classifying MIMIC-III discharge summaries and SEER cancer pathology reports. In our analysis, we show that two key components of BERT - pretraining and WordPiece tokenization - may actually be inhibiting BERT's performance on clinical text classification tasks where the input document is several thousand words long and where correctly identifying labels may depend more on identifying a few key words or phrases rather than understanding the contextual meaning of sequences of text.},
  keywords = {Adaptation models,BERT,Biological system modeling,Bit error rate,Cancer,clinical text,Data models,deep learning,MIMICs,natural language processing,neural networks,Task analysis,text classification},
  file = {/home/kydliceh/Zotero/storage/Z9XK9U2S/Gao et al. - 2021 - Limitations of Transformers on Clinical Text Class.pdf;/home/kydliceh/Zotero/storage/BKBGUV3H/9364676.html}
}

@article{haagsmaOverviewCrossGenreGender,
  title = {Overview of the {{Cross-Genre Gender Prediction Shared Task}} on {{Dutch}} at {{CLIN29}}},
  author = {Haagsma, Hessel and Kreutz, Tim and Medvedeva, Masha and Nissim, Malvina},
  abstract = {This overview presents the results of the cross-genre gender prediction task (GxG) organized at CLIN29. Teams were tasked with training a system to predict the gender of authors of tweets, YouTube comments and news articles. In the cross-genre setting, systems were trained on two genres, and tested on the other to assess domain adaptivity of the solutions. Eight teams participated in the shared task. Performance was generally better in the in-genre setting. In the cross-genre settings, performance on news articles declined the most compared to other target genres.},
  langid = {english},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/KYV9VWSK/Haagsma et al. - Overview of the Cross-Genre Gender Prediction Shar.pdf}
}

@article{haagsmaOverviewCrossGenreGendera,
  title = {Overview of the {{Cross-Genre Gender Prediction Shared Task}} on {{Dutch}} at {{CLIN29}}},
  author = {Haagsma, Hessel and Kreutz, Tim and Medvedeva, Masha and Nissim, Malvina},
  abstract = {This overview presents the results of the cross-genre gender prediction task (GxG) organized at CLIN29. Teams were tasked with training a system to predict the gender of authors of tweets, YouTube comments and news articles. In the cross-genre setting, systems were trained on two genres, and tested on the other to assess domain adaptivity of the solutions. Eight teams participated in the shared task. Performance was generally better in the in-genre setting. In the cross-genre settings, performance on news articles declined the most compared to other target genres.},
  langid = {english},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/37BL8YNE/Haagsma et al. - Overview of the Cross-Genre Gender Prediction Shar.pdf}
}

@inproceedings{howardUniversalLanguageModel2018a,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Howard, Jeremy and Ruder, Sebastian},
  year = {2018},
  month = jul,
  pages = {328--339},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1031},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.},
  file = {/home/kydliceh/Zotero/storage/PDQ92YD2/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf}
}

@misc{izmailovAveragingWeightsLeads2019,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = feb,
  number = {arXiv:1803.05407},
  eprint = {arXiv:1803.05407},
  publisher = {{arXiv}},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/9CWTMQFG/Izmailov et al. - 2019 - Averaging Weights Leads to Wider Optima and Better.pdf;/home/kydliceh/Zotero/storage/NZHYLTS5/1803.html}
}

@misc{joulinBagTricksEfficient2016,
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  year = {2016},
  month = aug,
  number = {arXiv:1607.01759},
  eprint = {arXiv:1607.01759},
  publisher = {{arXiv}},
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore\textasciitilde CPU, and classify half a million sentences among\textasciitilde 312K classes in less than a minute.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/GIFVVUBA/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf;/home/kydliceh/Zotero/storage/RTDQH4AV/1607.html}
}

@misc{joulinFastTextZipCompressing2016,
  title = {{{FastText}}.Zip: {{Compressing}} Text Classification Models},
  shorttitle = {{{FastText}}.Zip},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  year = {2016},
  month = dec,
  number = {arXiv:1612.03651},
  eprint = {arXiv:1612.03651},
  publisher = {{arXiv}},
  abstract = {We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/TN6HG5HL/Joulin et al. - 2016 - FastText.zip Compressing text classification mode.pdf;/home/kydliceh/Zotero/storage/YMJEXTD5/1612.html}
}

@inproceedings{karpovTransformerModelRetrosynthesis2019,
  title = {A {{Transformer Model}} for {{Retrosynthesis}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2019: {{Workshop}} and {{Special Sessions}}},
  author = {Karpov, Pavel and Godin, Guillaume and Tetko, Igor V.},
  editor = {Tetko, Igor V. and K{\r{u}}rkov{\'a}, V{\v e}ra and Karpov, Pavel and Theis, Fabian},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {817--830},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-30493-5_78},
  abstract = {We describe a Transformer model for a retrosynthetic reaction prediction task. The model is trained on 45~033 experimental reaction examples extracted from USA patents. It can successfully predict the reactants set for 42.7\% of cases on the external test set. During the training procedure, we applied different learning rate schedules and snapshot learning. These techniques can prevent overfitting and thus can be a reason to get rid of internal validation dataset that is advantageous for deep models with millions of parameters. We thoroughly investigated different approaches to train Transformer models and found that snapshot learning with averaging weights on learning rates minima works best. While decoding the model output probabilities there is a strong influence of the temperature that improves at \$\$\textbackslash text \{T\}=1.3\$\$the accuracy of models up~to 1\textendash 2\%.},
  isbn = {978-3-030-30493-5},
  langid = {english},
  keywords = {/unread,Character-based models,Computer aided synthesis planning,Retrosynthesis prediction,Transformer},
  file = {/home/kydliceh/Zotero/storage/PELYLQYK/Karpov et al. - 2019 - A Transformer Model for Retrosynthesis.pdf}
}

@misc{kirosSkipThoughtVectors2015,
  title = {Skip-{{Thought Vectors}}},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  year = {2015},
  month = jun,
  number = {arXiv:1506.06726},
  eprint = {arXiv:1506.06726},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.06726},
  abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/JPSIXXGD/Kiros et al. - 2015 - Skip-Thought Vectors.pdf;/home/kydliceh/Zotero/storage/7HBD5D62/1506.html}
}

@inproceedings{kupiecTrainableDocumentSummarizer1995,
  title = {A Trainable Document Summarizer},
  booktitle = {Proceedings of the 18th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Kupiec, Julian and Pedersen, Jan and Chen, Francine},
  year = {1995},
  month = jul,
  series = {{{SIGIR}} '95},
  pages = {68--73},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/215206.215333},
  isbn = {978-0-89791-714-8},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/ENJXVHAG/Kupiec et al. - 1995 - A trainable document summarizer.pdf}
}

@misc{leePatentBERTPatentClassification2019,
  title = {{{PatentBERT}}: {{Patent Classification}} with {{Fine-Tuning}} a Pre-Trained {{BERT Model}}},
  shorttitle = {{{PatentBERT}}},
  author = {Lee, Jieh-Sheng and Hsiang, Jieh},
  year = {2019},
  month = jun,
  number = {arXiv:1906.02124},
  eprint = {arXiv:1906.02124},
  publisher = {{arXiv}},
  abstract = {In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings. In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) a new state-of-the-art method based on pre-trained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/NSVG3SPN/Lee and Hsiang - 2019 - PatentBERT Patent Classification with Fine-Tuning.pdf;/home/kydliceh/Zotero/storage/D346NFQZ/1906.html}
}

@misc{liuMultiTaskDeepNeural2019,
  title = {Multi-{{Task Deep Neural Networks}} for {{Natural Language Understanding}}},
  author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  year = {2019},
  month = may,
  number = {arXiv:1901.11504},
  eprint = {arXiv:1901.11504},
  publisher = {{arXiv}},
  abstract = {In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7\% (2.2\% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at https://github.com/namisan/mt-dnn.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/8Z4BVZ4D/Liu et al. - 2019 - Multi-Task Deep Neural Networks for Natural Langua.pdf;/home/kydliceh/Zotero/storage/38VDFLD6/1901.html}
}

@misc{misraNewsCategoryDataset2022,
  title = {News {{Category Dataset}}},
  author = {Misra, Rishabh},
  year = {2022},
  month = oct,
  number = {arXiv:2209.11429},
  eprint = {arXiv:2209.11429},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.11429},
  abstract = {People rely on news to know what is happening around the world and inform their daily lives. In today's world, when the proliferation of fake news is rampant, having a large-scale and high-quality source of authentic news articles with the published category information is valuable to learning authentic news' Natural Language syntax and semantics. As part of this work, we present a News Category Dataset that contains around 210k news headlines from the year 2012 to 2022 obtained from HuffPost, along with useful metadata to enable various NLP tasks. In this paper, we also produce some novel insights from the dataset and describe various existing and potential applications of our dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Read},
  file = {/home/kydliceh/Zotero/storage/VFAA2XV2/Misra - 2022 - News Category Dataset.pdf;/home/kydliceh/Zotero/storage/K7N5SFM4/2209.html}
}

@misc{misraNewsHeadlinesDataset2022,
  title = {News {{Headlines Dataset For Sarcasm Detection}}},
  author = {Misra, Rishabh},
  year = {2022},
  month = sep,
  number = {arXiv:2212.06035},
  eprint = {arXiv:2212.06035},
  publisher = {{arXiv}},
  abstract = {Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag-based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets, and detecting sarcasm in these requires the availability of contextual tweets. To overcome the limitations related to noise in Twitter datasets, we curate News Headlines Dataset from two news websites: TheOnion aims at producing sarcastic versions of current events, whereas HuffPost publishes real news. The dataset contains about 28K headlines out of which 13K are sarcastic. To make it more useful, we have included the source links of the news articles so that more data can be extracted as needed. In this paper, we describe various details about the dataset and potential use cases apart from Sarcasm Detection.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Read},
  file = {/home/kydliceh/Zotero/storage/AXJWQ4YA/Misra - 2022 - News Headlines Dataset For Sarcasm Detection.pdf;/home/kydliceh/Zotero/storage/G8RD2DX3/2212.html}
}

@misc{NamsorNameChecker,
  title = {Namsor | {{Name}} Checker for {{Gender}}, {{Origin}} and {{Ethnicity}} Determination},
  abstract = {Namsor helps you to learn information about the gender, the country of origin and even the ethnicity of a name. Discover the origins of your name right now !},
  howpublished = {https://namsor.app/},
  langid = {english},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/ZMKNLZLG/namsor.app.html}
}

@misc{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  number = {arXiv:1802.05365},
  eprint = {arXiv:1802.05365},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.05365},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/Y8X9IQ4W/Peters et al. - 2018 - Deep contextualized word representations.pdf;/home/kydliceh/Zotero/storage/MNR8IF6D/1802.html}
}

@article{popelTrainingTipsTransformer2018,
  title = {Training {{Tips}} for the {{Transformer Model}}},
  author = {Popel, Martin and Bojar, Ond{\v r}ej},
  year = {2018},
  month = apr,
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume = {110},
  number = {1},
  eprint = {1804.00247},
  primaryclass = {cs},
  pages = {43--70},
  issn = {1804-0462},
  doi = {10.2478/pralin-2018-0002},
  abstract = {This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra "more data and larger models", we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/JSPDTGKU/Popel and Bojar - 2018 - Training Tips for the Transformer Model.pdf;/home/kydliceh/Zotero/storage/ZT3ZIRWK/1804.html}
}

@inproceedings{ruderTransferLearningNatural2019,
  title = {Transfer {{Learning}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Tutorials}}},
  author = {Ruder, Sebastian and Peters, Matthew E. and Swayamdipta, Swabha and Wolf, Thomas},
  year = {2019},
  month = jun,
  pages = {15--18},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-5004},
  abstract = {The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.},
  file = {/home/kydliceh/Zotero/storage/U9HDKXMZ/Ruder et al. - 2019 - Transfer Learning in Natural Language Processing.pdf}
}

@inproceedings{ruderTransferLearningNatural2019a,
  title = {Transfer {{Learning}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Tutorials}}},
  author = {Ruder, Sebastian and Peters, Matthew E. and Swayamdipta, Swabha and Wolf, Thomas},
  year = {2019},
  month = jun,
  pages = {15--18},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-5004},
  abstract = {The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.},
  file = {/home/kydliceh/Zotero/storage/CWU3HMJ3/Ruder et al. - 2019 - Transfer Learning in Natural Language Processing.pdf}
}

@article{seboPerformanceGenderDetection,
  title = {Performance of Gender Detection Tools: A Comparative Study of Name-to-Gender Inference Services},
  shorttitle = {Performance of Gender Detection Tools},
  author = {Sebo, Paul},
  journal = {Journal of the Medical Library Association : JMLA},
  volume = {109},
  number = {3},
  pages = {414--421},
  issn = {1536-5050},
  doi = {10.5195/jmla.2021.1185},
  abstract = {Objective: To evaluate the performance of gender detection tools that allow the uploading of files (e.g., Excel or CSV files) containing first names, are usable by researchers without advanced computer skills, and are at least partially free of charge. Methods: The study was conducted using four physician datasets (total number of physicians: 6,131; 50.3\% female) from Switzerland, a multilingual country. Four gender detection tools met the inclusion criteria: three partially free (Gender API, NamSor, and genderize.io) and one completely free (Wiki-Gendersort). For each tool, we recorded the number of correct classifications (i.e., correct gender assigned to a name), misclassifications (i.e., wrong gender assigned to a name), and nonclassifications (i.e., no gender assigned). We computed three metrics: the proportion of misclassifications excluding nonclassifications (errorCodedWithoutNA), the proportion of nonclassifications (naCoded), and the proportion of misclassifications and nonclassifications (errorCoded). Results: The proportion of misclassifications was low for all four gender detection tools (errorCodedWithoutNA between 1.5 and 2.2\%). By contrast, the proportion of unrecognized names (naCoded) varied: 0\% for NamSor, 0.3\% for Gender API, 4.5\% for Wiki-Gendersort, and 16.4\% for genderize.io. Using errorCoded, which penalizes both types of error equally, we obtained the following results: Gender API 1.8\%, NamSor 2.0\%, Wiki-Gendersort 6.6\%, and genderize.io 17.7\%. Conclusions: Gender API and NamSor were the most accurate tools. Genderize.io led to a high number of nonclassifications. Wiki-Gendersort may be a good compromise for researchers wishing to use a completely free tool. Other studies would be useful to evaluate the performance of these tools in other populations (e.g., Asian).},
  pmcid = {PMC8485937},
  pmid = {34629970},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/3H5VGP3R/Sebo - Performance of gender detection tools a comparati.pdf}
}

@misc{sidoCzertCzechBERTlike2021,
  title = {Czert -- {{Czech BERT-like Model}} for {{Language Representation}}},
  author = {Sido, Jakub and Pra{\v z}{\'a}k, Ond{\v r}ej and P{\v r}ib{\'a}{\v n}, Pavel and Pa{\v s}ek, Jan and Sej{\'a}k, Michal and Konop{\'i}k, Miloslav},
  year = {2021},
  month = aug,
  number = {arXiv:2103.13031},
  eprint = {arXiv:2103.13031},
  publisher = {{arXiv}},
  abstract = {This paper describes the training process of the first Czech monolingual language representation models based on BERT and ALBERT architectures. We pre-train our models on more than 340K of sentences, which is 50 times more than multilingual models that include Czech data. We outperform the multilingual models on 9 out of 11 datasets. In addition, we establish the new state-of-the-art results on nine datasets. At the end, we discuss properties of monolingual and multilingual models based upon our results. We publish all the pre-trained and fine-tuned models freely for the research community.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/C9GQ2XQA/Sido et al. - 2021 - Czert -- Czech BERT-like Model for Language Repres.pdf;/home/kydliceh/Zotero/storage/EEJBBSU9/2103.html}
}

@misc{smithCyclicalLearningRates2017a,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author = {Smith, Leslie N.},
  year = {2017},
  month = apr,
  number = {arXiv:1506.01186},
  eprint = {arXiv:1506.01186},
  publisher = {{arXiv}},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/kydliceh/Zotero/storage/3WWNYQFD/Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf;/home/kydliceh/Zotero/storage/LTKHNUQU/1506.html}
}

@misc{smithCyclicalLearningRates2017b,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author = {Smith, Leslie N.},
  year = {2017},
  month = apr,
  number = {arXiv:1506.01186},
  eprint = {arXiv:1506.01186},
  publisher = {{arXiv}},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/kydliceh/Zotero/storage/QSXJTGZP/Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf;/home/kydliceh/Zotero/storage/JSLNDWCW/1506.html}
}

@misc{smithCyclicalLearningRates2017c,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author = {Smith, Leslie N.},
  year = {2017},
  month = apr,
  number = {arXiv:1506.01186},
  eprint = {arXiv:1506.01186},
  publisher = {{arXiv}},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/kydliceh/Zotero/storage/NP43R4M4/Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf;/home/kydliceh/Zotero/storage/TQMTXVX4/1506.html}
}

@inproceedings{strakaSumeCzechLargeCzech2018a,
  title = {{{SumeCzech}}: {{Large Czech News-Based Summarization Dataset}}},
  shorttitle = {{{SumeCzech}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Straka, Milan and Mediankin, Nikita and Kocmi, Tom and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Hude{\v c}ek, Vojt{\v e}ch and Haji{\v c}, Jan},
  year = {2018},
  month = may,
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Miyazaki, Japan}},
  file = {/home/kydliceh/Zotero/storage/28AVDLW4/Straka et al. - 2018 - SumeCzech Large Czech News-Based Summarization Da.pdf}
}

@misc{sunHowFineTuneBERT2020,
  title = {How to {{Fine-Tune BERT}} for {{Text Classification}}?},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  year = {2020},
  month = feb,
  number = {arXiv:1905.05583},
  eprint = {arXiv:1905.05583},
  publisher = {{arXiv}},
  abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/CR77EHN7/Sun et al. - 2020 - How to Fine-Tune BERT for Text Classification.pdf;/home/kydliceh/Zotero/storage/KIDUR7G7/1905.html}
}

@misc{TransferLearningNatural,
  title = {Transfer {{Learning}} in {{Natural Language Processing}}},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/UMPYYQMN/Transfer Learning in Natural Language Processing.pdf}
}

@misc{UFALAIC,
  title = {{{UFAL AIC}}},
  howpublished = {https://aic.ufal.mff.cuni.cz/index.php/Main\_Page},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/FR7DASTD/Main_Page.html}
}

@misc{vaswaniAttentionAllYou2017d,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {arXiv:1706.03762},
  publisher = {{arXiv}},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/R23QINV5/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/kydliceh/Zotero/storage/SS66KQMA/1706.html}
}

@inproceedings{wangGLUEMultiTaskBenchmark2018,
  title = {{{GLUE}}: {{A Multi-Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  booktitle = {Proceedings of the 2018 {{EMNLP Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  year = {2018},
  pages = {353--355},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/W18-5446},
  abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
  langid = {english},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/Z5SY475F/Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf}
}

@misc{wangGLUEMultiTaskBenchmark2019,
  title = {{{GLUE}}: {{A Multi-Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  year = {2019},
  month = feb,
  number = {arXiv:1804.07461},
  eprint = {arXiv:1804.07461},
  publisher = {{arXiv}},
  abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/ZRXA3MRP/Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf;/home/kydliceh/Zotero/storage/K869Y53J/1804.html}
}

@inproceedings{wuMINDLargescaleDataset2020a,
  title = {{{MIND}}: {{A Large-scale Dataset}} for {{News Recommendation}}},
  shorttitle = {{{MIND}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Fangzhao and Qiao, Ying and Chen, Jiun-Hung and Wu, Chuhan and Qi, Tao and Lian, Jianxun and Liu, Danyang and Xie, Xing and Gao, Jianfeng and Wu, Winnie and Zhou, Ming},
  year = {2020},
  month = jul,
  pages = {3597--3606},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.331},
  abstract = {News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named MIND for news recommendation. Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body. We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. The MIND dataset will be available at https://msnews.github.io.},
  file = {/home/kydliceh/Zotero/storage/K6FSKJ2Q/Wu et al. - 2020 - MIND A Large-scale Dataset for News Recommendatio.pdf}
}

@article{zhangREVISITINGFEWSAMPLEBERT2021,
  title = {{{REVISITING FEW-SAMPLE BERT FINE-TUNING}}},
  author = {Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q and Artzi, Yoav},
  year = {2021},
  abstract = {This paper is a study of fine-tuning of BERT contextual representations, with focus on commonly observed instabilities in few-sample scenarios. We identify several factors that cause this instability: the common use of a non-standard optimization method with biased gradient estimation; the limited applicability of significant parts of the BERT network for down-stream tasks; and the prevalent practice of using a pre-determined, and small number of training iterations. We empirically test the impact of these factors, and identify alternative practices that resolve the commonly observed instability of the process. In light of these observations, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe the impact of these methods diminishes significantly with our modified process.},
  langid = {english},
  keywords = {/unread},
  file = {/home/kydliceh/Zotero/storage/UC9PNMX8/Zhang et al. - 2021 - REVISITING FEW-SAMPLE BERT FINE-TUNING.pdf}
}

@misc{zhangTextUnderstandingScratch2016,
  title = {Text {{Understanding}} from {{Scratch}}},
  author = {Zhang, Xiang and LeCun, Yann},
  year = {2016},
  month = apr,
  number = {arXiv:1502.01710},
  eprint = {arXiv:1502.01710},
  publisher = {{arXiv}},
  abstract = {This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/RI677NLV/Zhang and LeCun - 2016 - Text Understanding from Scratch.pdf;/home/kydliceh/Zotero/storage/BFXE6PPR/1502.html}
}

@misc{zhuangSurveyEfficientTraining2023,
  title = {A {{Survey}} on {{Efficient Training}} of {{Transformers}}},
  author = {Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
  year = {2023},
  month = feb,
  number = {arXiv:2302.01107},
  eprint = {arXiv:2302.01107},
  publisher = {{arXiv}},
  abstract = {Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/P7JBCFRI/Zhuang et al. - 2023 - A Survey on Efficient Training of Transformers.pdf;/home/kydliceh/Zotero/storage/DSC9XXRY/2302.html}
}
